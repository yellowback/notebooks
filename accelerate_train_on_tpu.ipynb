{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "accelerate-train-on-tpu.ipynb",
      "provenance": [],
      "mount_file_id": "1VahBRCT4GN4iPc2vp12Q53DED1OB-zo_",
      "authorship_tag": "ABX9TyNCct9AjDhOFEfbwVCLoWc1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yellowback/notebooks/blob/main/accelerate_train_on_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P6b3uqfzpDI"
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXs1MNqGBgnq",
        "outputId": "25a041d0-dd8e-49b7-f029-f276d2d69395"
      },
      "source": [
        "!pip install transformers accelerate datasets==1.11.0 fugashi ipadic"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Using cached transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "Collecting accelerate\n",
            "  Using cached accelerate-0.4.0-py3-none-any.whl (55 kB)\n",
            "Collecting datasets==1.11.0\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[K     |████████████████████████████████| 264 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting fugashi\n",
            "  Downloading fugashi-1.1.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (490 kB)\n",
            "\u001b[K     |████████████████████████████████| 490 kB 35.7 MB/s \n",
            "\u001b[?25hCollecting ipadic\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.4 MB 22.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (21.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 54.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (4.62.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (2.23.0)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 74.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.11.0) (1.1.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets==1.11.0) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.11.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.11.0) (3.0.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 42.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.11.0) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.11.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.11.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Building wheels for collected packages: ipadic\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556723 sha256=9a4373ae4720e2caa027c2fcdc8dc2704fa875ace5e9eef9a0ce18e31dc70221\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/8b/99/cf0d27191876637cd3639a560f93aa982d7855ce826c94348b\n",
            "Successfully built ipadic\n",
            "Installing collected packages: xxhash, tokenizers, sacremoses, pyyaml, huggingface-hub, fsspec, transformers, ipadic, fugashi, datasets, accelerate\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed accelerate-0.4.0 datasets-1.11.0 fsspec-2021.8.1 fugashi-1.1.1 huggingface-hub-0.0.17 ipadic-1.0.0 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUB12htcqU9W",
        "outputId": "a712180f-7919-446c-e797-9116135df712"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-xla==1.9\n",
            "  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 149.9 MB 30 kB/s \n",
            "\u001b[?25hCollecting cloud-tpu-client==0.10\n",
            "  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Installing collected packages: google-api-python-client, torch-xla, cloud-tpu-client\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 1.12.8\n",
            "    Uninstalling google-api-python-client-1.12.8:\n",
            "      Successfully uninstalled google-api-python-client-1.12.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "earthengine-api 0.1.278 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-xla-1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnuJzg4UCOhy",
        "outputId": "8a6f5469-92e6-426a-af8e-d40ffbe352ae"
      },
      "source": [
        "!git clone -b v4.10.0 https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 84334, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 84334 (delta 106), reused 131 (delta 76), pack-reused 84114\u001b[K\n",
            "Receiving objects: 100% (84334/84334), 67.52 MiB | 21.37 MiB/s, done.\n",
            "Resolving deltas: 100% (60524/60524), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su_UOljHlzwv"
      },
      "source": [
        "!ln -s /content/drive/MyDrive/colab/ldn ."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PcF8DGPxWDI",
        "outputId": "9ad9f568-f37a-416a-ebbe-cd21f87384cd"
      },
      "source": [
        "!accelerate config"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:Waiting for TPU to be start up with version pytorch-1.9...\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n",
            "In which compute environment are you running? ([0] This machine, [1] AWS (Amazon SageMaker)): 0\n",
            "Which type of machine are you using? ([0] No distributed training, [1] multi-CPU, [2] multi-GPU, [3] TPU): 3\n",
            "What is the name of the function in your script that should be launched in all parallel scripts? [main]: \n",
            "How many processes in total will you use? [1]: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aczvQWo5k5EF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa707eb4-916d-4183-b041-bdf6248c80a6"
      },
      "source": [
        "MODEL=\"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "OUTDIR=\"./output\"\n",
        "BS=8\n",
        "SEQLEN=512\n",
        "EPOCH=5\n",
        "!accelerate launch ./transformers/examples/pytorch/text-classification/run_glue_no_trainer.py \\\n",
        "       --model_name_or_path {MODEL} \\\n",
        "       --output_dir {OUTDIR} \\\n",
        "       --per_device_train_batch_size {BS} \\\n",
        "       --per_device_eval_batch_size {BS} \\\n",
        "       --max_length {SEQLEN} \\\n",
        "       --train_file ldn/data/train.csv \\\n",
        "       --validation_file ldn/data/dev.csv \\\n",
        "       --num_train_epochs {EPOCH}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n",
            "WARNING:root:TPU has started up successfully with version pytorch-1.9\n",
            "09/16/2021 00:13:10 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: xla:1\n",
            "Use FP16 precision: False\n",
            "\n",
            "09/16/2021 00:13:10 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 7\n",
            "Local process index: 7\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "09/16/2021 00:13:11 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 3\n",
            "Local process index: 3\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "09/16/2021 00:13:11 - WARNING - datasets.builder - Using custom data configuration default-b8cb270bea1140cf\n",
            "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-b8cb270bea1140cf/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff...\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-b8cb270bea1140cf/0.0.0/9144e0a4e8435090117cea53e6c7537173ef2304525df4a077c435d8ee7828ff. Subsequent calls will reuse this data.\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 139809621450128 acquired on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpw6q4y8lq\n",
            "Downloading: 100% 479/479 [00:00<00:00, 347kB/s]\n",
            "storing https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "creating metadata file for /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 139809621450128 released on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\",\n",
            "    \"4\": \"LABEL_4\",\n",
            "    \"5\": \"LABEL_5\",\n",
            "    \"6\": \"LABEL_6\",\n",
            "    \"7\": \"LABEL_7\",\n",
            "    \"8\": \"LABEL_8\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3,\n",
            "    \"LABEL_4\": 4,\n",
            "    \"LABEL_5\": 5,\n",
            "    \"LABEL_6\": 6,\n",
            "    \"LABEL_7\": 7,\n",
            "    \"LABEL_8\": 8\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140417438758096 acquired on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140417438758096 released on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140232340447504 acquired on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140232340447504 released on /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 139809621415696 acquired on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_fisybzs\n",
            "Downloading: 100% 110/110 [00:00<00:00, 95.1kB/s]\n",
            "storing https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc\n",
            "creating metadata file for /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 139809621415696 released on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140417335404368 acquired on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140417335404368 released on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140232237098640 acquired on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "09/16/2021 00:13:13 - INFO - filelock - Lock 140232237098640 released on /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc.lock\n",
            "loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 140232340419344 acquired on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "Downloading: 100% 258k/258k [00:00<00:00, 2.12MB/s]\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 140232340419344 released on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 139809451936144 acquired on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 139809451936144 released on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 140417438717648 acquired on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "09/16/2021 00:13:14 - INFO - filelock - Lock 140417438717648 released on /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4.lock\n",
            "loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/15164357d71cd32532e56c1d7c2757141326ae17c53e2277bc417cc7c21da6ea.a7378a0cbee5cff668832a776d72b97a25479604fe9564d5595897f75049e7f4\n",
            "loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/0e46f722799f19c3f0c53172545108a4b31847d3b9a2d5b100759f6673bd667b.08ae4e4044742b9cc7172698caf1da2524f5597ff8cf848114dd0b730cc44bdc\n",
            "loading file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/tokenizer.json from cache at None\n",
            "loading configuration file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/573af37b6c39d672f2df687c06ad7d556476cbe43e5bf7771097187c45a3e7bf.abeb707b5d79387dd462e8bfb724637d856e98434b6931c769b8716c6f287258\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertJapaneseTokenizer\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "09/16/2021 00:13:15 - INFO - filelock - Lock 140232208129040 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "Downloading:   7% 33.2M/445M [00:01<00:15, 27.2MB/s]09/16/2021 00:13:16 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 4\n",
            "Local process index: 4\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "Downloading:  14% 60.5M/445M [00:01<00:11, 34.7MB/s]09/16/2021 00:13:17 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 6\n",
            "Local process index: 6\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "Downloading:  14% 64.0M/445M [00:02<00:12, 30.7MB/s]09/16/2021 00:13:17 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 5\n",
            "Local process index: 5\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "Downloading:  18% 79.7M/445M [00:02<00:13, 26.9MB/s]09/16/2021 00:13:18 - INFO - run_glue_no_trainer - Distributed environment: TPU\n",
            "Num processes: 8\n",
            "Process index: 2\n",
            "Local process index: 2\n",
            "Device: xla:0\n",
            "Use FP16 precision: False\n",
            "\n",
            "Downloading: 100% 445M/445M [00:15<00:00, 28.2MB/s]\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 140232208129040 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 140631488577168 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 140631488577168 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139790016303056 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139790016303056 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139640973623056 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139640973623056 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139986335215184 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139986335215184 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139809329760464 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 139809329760464 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "loading weights file https://huggingface.co/cl-tohoku/bert-base-japanese-whole-word-masking/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 140417297878992 acquired on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "09/16/2021 00:13:31 - INFO - filelock - Lock 140417297878992 released on /root/.cache/huggingface/transformers/cabd9bbd81093f4c494a02e34eb57e405b7564db216404108c8e8caf10ede4fa.464b54997e35e3cc3223ba6d7f0abdaeb7be5b7648f275f57d839ee0f95611fb.lock\n",
            "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset: 100% 6/6 [03:18<00:00, 33.05s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:18<00:00, 33.10s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:19<00:00, 33.19s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:18<00:00, 33.12s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:19<00:00, 33.18s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:19<00:00, 33.31s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:20<00:00, 33.36s/ba]\n",
            "Running tokenizer on dataset: 100% 6/6 [03:19<00:00, 33.28s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:49<00:00, 24.65s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:49<00:00, 24.50s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:48<00:00, 24.45s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:49<00:00, 24.52s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:49<00:00, 24.55s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:48<00:00, 24.38s/ba]\n",
            "09/16/2021 00:17:58 - INFO - run_glue_no_trainer - Sample 982 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 4009, 11, 24812, 34, 9224, 14, 5924, 2156, 4836, 7, 2906, 679, 101, 37, 17, 32, 221, 1174, 13, 139, 450, 63, 273, 9943, 9528, 65, 5, 10710, 13, 9368, 2148, 14, 1174, 26, 20, 10, 8, 63, 10932, 35, 6122, 1433, 25, 65, 5, 1678, 35, 1130, 1662, 734, 14, 6, 63, 23767, 65, 731, 5, 3435, 869, 7055, 4682, 5, 18435, 11, 4702, 8, 108, 90, 5, 2387, 9, 6, 18715, 18, 7657, 8425, 1637, 8, 31, 32, 6, 12101, 2405, 104, 3826, 11, 9914, 10, 2180, 12, 483, 19, 174, 5, 1040, 5, 1925, 11, 1296, 34, 8, 5359, 7, 962, 31, 13215, 18, 3290, 12, 10030, 1040, 14, 7033, 2180, 7, 17002, 16, 33, 5, 29, 8, 1195, 11, 8935, 1165, 10, 8834, 6, 8425, 1637, 5, 6988, 5, 1850, 9, 927, 28475, 20, 1165, 6, 218, 126, 5, 5820, 14, 1195, 757, 28715, 28491, 20, 16, 21, 10, 120, 12, 31, 45, 11, 4841, 8, 1570, 28, 7864, 28, 11353, 255, 6, 1040, 6155, 28, 21646, 45, 5, 203, 80, 9555, 29460, 18, 7097, 1032, 5, 51, 6, 306, 14, 1330, 16, 21, 10, 5, 9, 14556, 81, 18, 1360, 1781, 13, 1429, 11, 8069, 28449, 22684, 2203, 31, 8584, 1964, 308, 10, 8, 1195, 5, 2203, 11753, 1964, 7, 1878, 28724, 28532, 72, 6, 6861, 5, 1040, 13, 6904, 5, 10678, 11, 4841, 8, 2387, 8425, 1637, 11, 6392, 5, 9, 6, 63, 165, 4393, 4952, 65, 731, 5, 6558, 21579, 605, 12, 6047, 5, 969, 11, 1, 7, 15, 10, 12303, 28472, 21994, 590, 8, 6047, 11, 24812, 34, 20359, 4344, 84, 10, 9224, 9, 6, 324, 4399, 2868, 5, 5266, 11, 830, 9214, 12, 8766, 28468, 10, 120, 8, 108, 90, 12, 9, 6, 59, 9224, 11, 8368, 6, 25709, 3350, 5, 4836, 11, 1714, 24584, 12056, 16, 33, 8, 7168, 6, 1174, 26, 20, 10, 9368, 2148, 12, 9, 6, 27575, 7, 16893, 4836, 3169, 11, 13421, 28584, 28674, 28449, 45, 14, 392, 8, 871, 19, 6, 286, 5, 1984, 1652, 1808, 14, 7454, 2203, 1113, 22536, 18, 594, 1964, 97, 25, 247, 7, 2211, 20, 10, 12303, 8, 63, 23767, 65, 731, 5, 11185, 28472, 6357, 1741, 28, 2679, 29238, 28517, 5, 4836, 17594, 5, 13687, 12, 6, 8763, 5, 2203, 548, 29992, 384, 1964, 1302, 50, 151, 324, 14, 2906, 34, 451, 13, 58, 16, 33, 8, 108, 90, 12, 5924, 2156, 6, 8763, 5, 15273, 7446, 143, 17, 1302, 14, 18517, 1088, 29460, 4836, 9, 547, 28674, 75, 8, 450, 63, 273, 9943, 9528, 65, 9, 6, 101, 37, 17, 32, 23, 412, 24, 221, 22245, 14739, 15161, 825, 1169, 3589, 6797, 8, 35, 450, 63, 273, 9943, 9528, 65, 61, 1529, 2180, 9680, 10594, 28838, 12862, 12377, 12202, 73, 28484, 28484, 28506, 2622, 9594, 35, 7755, 36, 5285, 28673, 28564, 38, 731, 5, 1789, 1989, 14, 1169, 2000, 12, 35, 4497, 3443, 29986, 6, 10077, 869, 4587, 6014, 7, 1320, 35, 2007, 29293, 3915, 2976, 6, 8723, 602, 15, 10, 3196, 29331, 1052, 7, 36, 5465, 9, 3], 'labels': 4, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/16/2021 00:17:58 - INFO - run_glue_no_trainer - Sample 1993 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 11475, 221, 9960, 1514, 1350, 28472, 17350, 9, 6, 4691, 21522, 834, 2035, 1645, 36, 9960, 8761, 38, 11, 2910, 34, 8, 9960, 8761, 12, 5, 2678, 230, 2313, 7319, 5, 7317, 28, 779, 14, 6, 97, 52, 1406, 50, 1571, 3527, 1091, 11483, 5, 1213, 14, 7122, 1473, 11, 3723, 34, 36, 11483, 2448, 7786, 3057, 29399, 38, 5, 7317, 14, 343, 62, 8, 7317, 1799, 9, 7786, 14, 1122, 34, 626, 32, 126, 75, 8, 70, 1645, 9, 17, 198, 9959, 1208, 1840, 9354, 666, 34, 45, 14, 203, 6, 17, 32, 9959, 48, 198, 126, 4673, 14, 519, 75, 8, 666, 34, 82, 7, 9, 6, 36, 9960, 8761, 38, 3237, 1800, 14, 727, 12, 4871, 9, 4691, 8, 521, 36, 9960, 8761, 38, 14, 666, 519, 7, 139, 5, 9, 562, 75, 8, 391, 815, 12806, 5, 9960, 28451, 28478, 14719, 23, 482, 27598, 2413, 222, 26881, 28451, 9966, 28513, 7694, 23, 775, 2413, 222, 24835, 35, 5348, 13442, 8726, 6, 148, 15154, 23, 482, 1241, 2413, 24, 5, 420, 2413, 8, 15650, 2239, 900, 7, 10265, 9285, 61, 16325, 9, 205, 28461, 485, 120, 8, 1499, 1545, 15, 16, 33, 9960, 1514, 1350, 5867, 250, 1645, 9, 7021, 9482, 89, 962, 10265, 45, 28, 130, 3200, 34, 5, 12, 9, 80, 3635, 205, 8, 9960, 8761, 25035, 1634, 2622, 35, 10247, 14, 15273, 5087, 25340, 10798, 1269, 5048, 6570, 36, 19228, 61, 4253, 1949, 38, 11, 3723, 9680, 4459, 9594, 35, 6093, 16562, 14, 10290, 7, 7008, 386, 29563, 679, 2672, 12, 5, 1011, 4675, 6653, 19927, 9680, 4459, 9594, 35, 10731, 28, 656, 12, 5552, 14, 221, 24804, 17230, 1276, 28463, 1428, 7, 10731, 14, 18060, 7397, 29, 9680, 2626, 9594, 35, 9680, 2622, 10072, 9594, 350, 21694, 28550, 17, 676, 17, 5, 121, 2604, 25317, 18570, 5, 90, 29002, 9680, 3030, 10740, 28743, 2345, 9594, 35, 73, 19, 29404, 7, 9, 450, 11, 10439, 2935, 91, 176, 679, 28496, 450, 413, 19, 29404, 28909, 38, 9, 21865, 758, 9680, 4459, 9594, 3], 'labels': 2, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "09/16/2021 00:17:58 - INFO - run_glue_no_trainer - Sample 2862 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [2, 36, 108, 28486, 5, 27966, 9, 4871, 3286, 36, 3246, 5, 5419, 2992, 3286, 12272, 2621, 1396, 12, 6905, 49, 18332, 12, 2612, 1925, 49, 98, 28611, 5, 5419, 11, 3091, 34, 11660, 9, 6, 12597, 6905, 64, 11, 546, 16, 33, 13, 3733, 13, 1281, 16, 2575, 1101, 71, 16, 2501, 5797, 12, 31, 8, 218, 11, 546, 16, 2093, 9, 36, 18178, 28454, 1778, 38, 36, 15128, 28478, 28478, 3286, 64, 13, 2512, 15, 6, 106, 15137, 14, 3370, 13, 2612, 108, 11, 1418, 15, 790, 6, 5419, 2869, 11, 5197, 15, 790, 140, 2203, 17377, 1964, 7, 139, 3876, 28, 707, 8, 854, 81, 7, 6905, 12, 2612, 49, 5419, 11, 14537, 28484, 5, 9, 6, 12597, 3952, 16, 8682, 5, 4538, 13, 58, 16, 33, 972, 9, 3727, 203, 80, 3635, 205, 8, 373, 6, 12597, 12, 332, 16, 28, 10580, 5, 2612, 11, 3091, 15, 790, 6, 5419, 11, 14537, 28454, 790, 140, 15736, 9, 569, 335, 11, 2130, 6, 12959, 33, 8, 8871, 218, 230, 2203, 1571, 2612, 2057, 2068, 1964, 50, 131, 7, 1054, 34, 140, 53, 28, 3731, 16, 9, 33, 14, 6, 378, 5, 53, 14, 909, 4840, 7, 1221, 11, 9076, 16, 33, 3133, 12, 9, 80, 8, 9404, 29875, 1950, 5, 8313, 28601, 2375, 23, 965, 573, 35, 569, 335, 24, 28, 167, 12, 2612, 11, 13297, 13, 6, 6905, 7, 1925, 11, 3091, 34, 5, 14, 32, 29435, 140, 52, 53, 75, 8, 8313, 28601, 2375, 5, 2612, 9, 20613, 27073, 6, 16503, 3744, 6, 4754, 28700, 6, 26771, 6, 24386, 10531, 7911, 5, 3614, 5309, 28607, 14, 5394, 6, 2612, 5, 1232, 6154, 28, 8559, 28555, 28479, 8, 73, 14318, 49, 20039, 7099, 1263, 28, 21160, 28563, 28507, 12, 6, 16653, 2612, 5, 108, 7, 71, 16, 2501, 124, 18, 14699, 26, 8, 36, 4699, 2612, 14, 3596, 12, 6, 27794, 28, 3596, 8, 2612, 5, 5185, 1720, 5, 45, 11, 4660, 5034, 16, 6, 13729, 28340, 5, 466, 7, 1281, 16, 21, 10, 45, 28, 130, 2610, 38, 23, 8313, 28601, 2375, 24, 373, 5356, 28653, 7, 8282, 13, 6, 2198, 14, 2764, 16, 18798, 16, 33, 32, 28, 31, 4627, 18, 5602, 1691, 3904, 1040, 5, 82, 7, 14699, 18, 27966, 11, 17461, 16, 6905, 7, 3091, 34, 8313, 28601, 2375, 5, 8805, 26, 5, 6043, 13457, 9, 6372, 5359, 40, 2501, 5, 29, 2935, 140, 6632, 14, 19315, 16, 2501, 8, 36, 17, 53, 5144, 75, 13, 27075, 7563, 13632, 30148, 28911, 3379, 16, 28, 2949, 16, 8009, 53, 14, 21, 332, 16, 17830, 485, 40, 6, 3654, 29, 7, 212, 16, 9182, 6172, 9366, 5, 9, 130, 2610, 8, 5408, 30054, 28477, 5416, 7, 63, 7613, 28454, 1778, 3158, 10294, 63, 1325, 28, 2949, 1549, 3158, 10294, 2977, 16, 8009, 13, 205, 28461, 485, 2992, 8, 1325, 5, 344, 9, 17894, 2612, 14, 5207, 75, 40, 2612, 11, 3091, 15, 16, 33, 11218, 6, 892, 9404, 29875, 1950, 5, 53, 75, 13, 6, 18124, 63, 27075, 7563, 9037, 16458, 16, 17461, 10, 40, 3], 'labels': 0, 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Running tokenizer on dataset: 100% 2/2 [00:49<00:00, 24.58s/ba]\n",
            "Running tokenizer on dataset: 100% 2/2 [00:47<00:00, 23.68s/ba]\n",
            "Downloading: 3.21kB [00:00, 1.42MB/s]       \n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer - ***** Running training *****\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Num examples = 5893\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Num Epochs = 5\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Instantaneous batch size per device = 8\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Gradient Accumulation steps = 1\n",
            "09/16/2021 00:18:28 - INFO - run_glue_no_trainer -   Total optimization steps = 465\n",
            " 20% 93/465 [04:45<36:47,  5.93s/it]09/16/2021 00:23:34 - INFO - run_glue_no_trainer - epoch 0: {'accuracy': 0.85546875}\n",
            " 40% 186/465 [06:21<03:29,  1.33it/s]09/16/2021 00:25:03 - INFO - run_glue_no_trainer - epoch 1: {'accuracy': 0.9192708333333334}\n",
            " 60% 279/465 [07:50<02:17,  1.35it/s]09/16/2021 00:26:32 - INFO - run_glue_no_trainer - epoch 2: {'accuracy': 0.9381510416666666}\n",
            " 80% 372/465 [10:42<01:13,  1.27it/s]09/16/2021 00:29:24 - INFO - run_glue_no_trainer - epoch 3: {'accuracy': 0.9446614583333334}\n",
            "100% 465/465 [12:10<00:00,  1.39it/s]09/16/2021 00:30:52 - INFO - run_glue_no_trainer - epoch 4: {'accuracy': 0.9453125}\n",
            "Configuration saved in ./output/config.json\n",
            "Model weights saved in ./output/pytorch_model.bin\n",
            "100% 465/465 [12:29<00:00,  1.61s/it]\n"
          ]
        }
      ]
    }
  ]
}